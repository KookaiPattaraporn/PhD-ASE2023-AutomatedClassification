\section{Background and Related Work} \label{sec:related-work}

\newtext{Personal data protection and privacy have attracted attention from individuals and organisations globally in recent years. After the announcement of GDPR in 2016, over hundred countries around the world have developed their own data protection and privacy legislations \cite{UNCTAD2020}. As people interact with software applications and systems, it is necessary to develop the software applications and systems that comply with those legislations to ensure personal data protection.} However, it is challenging for software engineers to understand and implement privacy requirements when developing software applications in practice \cite{Gurses2011}. 

Several work identified privacy requirements and constructed privacy requirement taxonomies based on privacy policies and privacy standards \cite{Antn2004,Ayala-Rivera2018}. \citeauthor{Antn2004} \cite{Antn2004} developed a privacy goal taxonomy from website privacy policies. The study adopted a Goal-based Requirement Analysis Method (GBRAM) to extract privacy goals and requirements from 24 online healthcare privacy policies. This taxonomy provides a useful set of requirements that the website developers could use to reduce web vulnerabilities. \citeauthor{Ayala-Rivera2018} \cite{Ayala-Rivera2018} proposed an approach to identify requirements that should be implemented in a software system. The requirements were elicited from the mapping between GDPR and privacy controls in ISO/IEC standards. \newtext{Although both studies are relevant to privacy requirements elicitation and classification, none of them investigated privacy requirements in issue reports and studied on the automated classification process.}

\newtext{A number of existing studies used ML and NLP methods to assist in regulatory compliance checking \cite{Muller2019,Torre}. \citeauthor{Muller2019} \cite{Muller2019} studied on a method used to check the compliance between GDPR and companies' privacy policies. The statements in privacy policies were extracted and classified into five categories (i.e. Data Protection Officer (DPO), Purpose, Acquired data, Data sharing and Rights). The study applied three different word embedding techniques (i.e. Word2Vec, FastText \cite{Bojanowski2017} and ELMo \cite{Peters2018}) in combination with three classifiers (i.e. Support Vector Machines (SVMs), Logistic Regression (LR) and Neural Networks (NN)) to automatically classify the extracted privacy policies' statements into those five categories. It employed F-measure to evaluate the classification performance. Similarly, \citeauthor{Torre} \cite{Torre} proposed a solution for completeness checking of privacy policies against GDPR. The study used a pre-trained word-vector model to transform sentences in privacy policies into vector representations \cite{Pennington2014}. It then built ML classifiers for classifying the sentences into metadata types. Precision and recall were used as performance measures. Comparing to our study, these studies focused on privacy policies, employed fewer word embedding techniques and classified data into a smaller group of categories.}

\newtext{A number of studies applied ML, NLP and deep learning models to perform issue report classification in different applications \cite{Fan2017b,Pandey2017,Choetkiertikul,Cho2022}. \citeauthor{Fan2017b} \cite{Fan2017b} performed a large-scale study on issue reports of 80 popular projects in GitHub. They classified issues into a bug or non-bug type. The study evaluated the classification performance of four traditional text-based classifiers which are SVM, LR, Multinomial Naive Bayes (MNB) and Random Forest (RF). It used an average F-measure to evaluate the classification performance of those classifiers. It also proposed a new framework that can improve the performances of the traditional classification methods. \citeauthor{Pandey2017} \cite{Pandey2017} studied various classification algorithms used to classify issue reports of three open-source software projects based on their types (i.e. bug or improvement). The algorithms consist of Naive Bayes (NB), linear discriminant analysis, k-nearest neighbours and SVM with various kernels, decision tree and RF. The authors employed F-measure, average accuracy and weighted average F-measure to evaluate the classification performance. The study reported that RF performed best in this setting.}

\newtext{\citeauthor{Choetkiertikul} \cite{Choetkiertikul} proposed a predictive model that can be used to recommend the most relevant software components for new issue reports. The model was built using the deep learning Long Short-Term Memory (LSTM) technique. The issue reports from a collection of 11 open-source projects from four repositories were used as inputs for the model. The performance of the model was evaluated using recall@k. \citeauthor{Cho2022} \cite{Cho2022} proposed a method that automatically classified issue reports into software feature descriptions in user manuals of three software projects: Notepad++, Visual Studio Code and Komodo. The authors performed an experiment to evaluate the classification performance of 8 approaches based on a combination of two deep learning models (i.e. CNN and Recurrent Neural Network (RNN)) and four word embedding techniques (i.e. embedding layer, Word2vec, GloVe and FastText). The study used precision, recall and F-measure to evaluate the performance of those approaches. The best performing word embedding techniques are FastText and Glove while CNN performed better than RNN in this experimental setting. Although the existing studies mentioned above investigated various classification problems in issue reports, they focused on types/components classification, not privacy-related issue reports. In addition, our study employed some different word embedding techniques, deep learning models and performance measures compared to those studies.}

\newtext{Recent work \cite{Sangaroonsilp2023} developed a taxonomy of privacy requirements aiming to support the development of privacy-aware software systems. The work identified privacy requirements in issue reports and mapped them to relevant privacy requirements in the taxonomy. The process of taxonomy development consists of three main steps: privacy requirements identification, refinement and classification. In the privacy requirements identification step, the privacy requirements were derived from four well-established data protection regulations and privacy frameworks (i.e. General Data Protection Regulations (GDPR) \cite{OfficeJournaloftheEuropeanUnion;2016}, Thailand Personal Data Protection Act (PDPA) \cite{PDPA}, ISO/IEC 29100 \cite{ISO/IEC2011} and Asia-Pacific Economic Cooperation (APEC) privacy framework \cite{Apec2015}).}

\newtext{GDPR is a data protection regulation developed by the European Union (EU). It provides a set of principles and conditions for protecting personal data and individual rights which need to be complied by applicable organisations. Thailand PDPA, based on GDPR, is a country-specific personal data protection regulation that governs the use and protection of personal data. ISO/IEC 29100 and APEC privacy framework define a set of privacy principles used to manage personal data processing activities. They also provide guidelines for controlling the processing of personal data. The processing of personal data includes collection, use, storage, dissemination and disposal.}

\newtext{The privacy requirements derived from multiple sources can be redundant and/or inconsistent, thus the privacy requirements refinement step was designed to remove duplicate requirements and manage inconsistent requirements. These privacy requirements were then classified into categories in the privacy requirements classification step. In this step, the authors identified the privacy requirements that have common characteristics or address similar privacy concerns, grouped those privacy requirements together, and finally formed the categories. This created a taxonomy of 71 privacy requirements with 7 categories which consist of (1) user participation, (2) notice, (3) user desirability, (4) data processing, (5) breach, (6) complaint/request and (7) security.}

\newtext{To validate the taxonomy, the authors of the paper performed a study on how issue reports of two large open-source software projects (Google Chrome and Moodle) address the privacy requirements in the taxonomy. Google Chrome and Moodle projects were selected due to their accessibility of issue reports, large scale size, popularity and representativeness. To identify and classify privacy requirements in issue reports, the authors first collected issue reports from issue tracking systems of Chrome and Moodle. There were 896 Chrome and 478 Moodle issue reports collected. Those issue reports were marked as \emph{closed} and \emph{privacy-related}.}

\newtext{Then, the issue classification process was performed by three coders, who were the authors of the paper and also involved in the taxonomy development process. Each issue report was annotated by two coders. The coders followed the following three steps to classify issue reports to relevant privacy requirements. First, the coders identified concerned personal data (e.g. name, email address and bank account details) described in an issue report. Then, they identified functions/properties related to the concerned personal data reported in the issue. Finally, they mapped the issue report to one or more relevant privacy requirements. The coder took approximately 138 person-hours to classify 1,374 issue reports. The coders also performed an inter-rater reliability assessment and a disagreement resolution to mitigate subjective judgements in issue report classification process. This process produced a dataset that contains Google Chrome and Moodle issue reports associated with concerned privacy requirements.}

\newtext{Although the work in \cite{Sangaroonsilp2023} provides a valuable dataset for mining and classifying privacy requirements in issue reports, an \emph{automated approach} has not been studied. As can be seen above, the issue classification process is labour intensive and time-consuming. Thus, we aim to automate this process by exploring and evaluating multiple feature extraction techniques, and reporting the best technique that can be used in this setting. The automated approach would reduce the use of resources and efforts as well as minimise human errors.}